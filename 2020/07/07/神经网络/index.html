<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="华中科技大学17级本科生">
  <meta name="author" content="XIN">
  <meta name="keywords" content="">
  <title>神经网络 - XIN&#39;S BLOG</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/androidstudio.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->
<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">

<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">

<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>XIN'S BLOG</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/code.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2020-07-07 20:56">
      2020年7月7日 晚上
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      2.4k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      28
       分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h1 id="1-神经网络基础"><a href="#1-神经网络基础" class="headerlink" title="1. 神经网络基础"></a>1. 神经网络基础</h1><p>神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。对于普通神经网络，最普通的层的类型是全连接层。全连接层中的神经元与其前后两层的神经元是全连接的。神经网络包括输入层、隐藏层、输出层，一般说N层神经网络的时候，忽略了输入层，比如下图是一个2层神经网络。<strong>输入层神经元节点的数量就是数据feature的数量。</strong></p>
<p><img src="https://gitee.com/huster_ning/image/raw/master//image/20200707210313.png" srcset="/img/loading.gif" alt=""></p>
<p>一个神经元节点首先计算线性函数（$\omega x + b$），然后计算激活函数，然后将计算的结果传递到下一层。</p>
<p>训练神经网络的过程：</p>
<p>1） 参数的随机初始化；</p>
<p>2） 前向传播；</p>
<p>3） 计算代价；</p>
<p>4）反向传播计算所有偏导数；</p>
<p>5） 利用数值检验方法检验这些偏导数；</p>
<p>6）使用优化来最小化代价函数。</p>
<p>用上图的2层神经网络举例说明<strong>前向传播</strong>的过程，输入是一个<code>3 x 1</code>的向量，第一个隐藏层的权重W1是<code>4 x 3</code>的向量,偏置b1是<code>4 x 1</code>的向量，在隐藏层中的计算为：$f(W_1 x + b_1)$，其中输出数据大小为<code>4 x 1</code>；输出层的权重W2是<code>2 x 4</code>, 偏置为<code>2 x 1</code>, 输出层通常没有激活函数，则计算为：$W_2 x + b_2$, 最后输出数据为<code>2 x 1</code>的向量。</p>
<h2 id="1-1-数据预处理"><a href="#1-1-数据预处理" class="headerlink" title="1.1 数据预处理"></a>1.1 数据预处理</h2><p>假设输入数据为X，大小为<code>n x m</code>，其中n指的特征的数量，m指的是样本的数量。</p>
<h3 id="1-1-1-均值减法（Mean-subtraction）"><a href="#1-1-1-均值减法（Mean-subtraction）" class="headerlink" title="1.1.1 均值减法（Mean subtraction）"></a>1.1.1 均值减法（Mean subtraction）</h3><p>对数据每个独立特征减去平均值（<strong>注意：平均值指的是每个样本对应特征的平均值，而不是对单个样本的数据求平均值</strong>）, 其实就是在每个维度上将数据的中心移到原点，也就是做0中心化（zero-centered）处理。</p>
<div class="hljs"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
X -= np.mean(X, axis=<span class="hljs-number">1</span>)</code></pre></div>
<h3 id="1-1-2-归一化-Normalization"><a href="#1-1-2-归一化-Normalization" class="headerlink" title="1.1.2 归一化 (Normalization)"></a>1.1.2 归一化 (Normalization)</h3><p>有两种方法：</p>
<ul>
<li>先对数据做0中心化，然后每个维度都除以其标准差。</li>
</ul>
<div class="hljs"><pre><code class="hljs undefined">X <span class="hljs-string">/=</span> np.std<span class="hljs-params">(X, <span class="hljs-attr">axis</span>=1)</span></code></pre></div>
<ul>
<li>对每个维度都做归一化。</li>
</ul>
<p><strong>常见错误</strong>：<strong>数据预处理只能在训练集数据上进行计算</strong>。比如，先从训练集中求图片平均值，然后训练集、验证集、测试集中的图像再减去这个平均值。</p>
<h2 id="1-2-随机初始化"><a href="#1-2-随机初始化" class="headerlink" title="1.2 随机初始化"></a>1.2 随机初始化</h2><h3 id="1-2-1-权重初始化"><a href="#1-2-1-权重初始化" class="headerlink" title="1.2.1 权重初始化"></a>1.2.1 权重初始化</h3><ul>
<li>小随机数初始化</li>
</ul>
<div class="hljs"><pre><code class="hljs python">W = np.random.randn(D, H) * <span class="hljs-number">0.01</span></code></pre></div>
<ul>
<li>使用<code>1/sqrt(m)</code>校准方差：这样可以将神经元输出归一化到1：<code>W = np.random.randn(m) / sqrt(m)</code>， m为样本数量。针对ReLU神经元的特殊初始化：<code>w = np.random.randn(m) * sqrt(2.0 / m)</code></li>
</ul>
<h3 id="1-2-2-偏置初始化"><a href="#1-2-2-偏置初始化" class="headerlink" title="1.2.2 偏置初始化"></a>1.2.2 偏置初始化</h3><p>通常将偏置初始化为0，这是因为权重的随机初始化已经打破了对称性。</p>
<h2 id="1-3-学习之前的合理性检查"><a href="#1-3-学习之前的合理性检查" class="headerlink" title="1.3 学习之前的合理性检查"></a>1.3 学习之前的合理性检查</h2><ul>
<li>寻找特定情况下的正确损失值，比如使用一个样本，正则化强度设置为0，检查损失值是否符合预期。</li>
<li>提高正则化强度时导致损失值变大；</li>
<li>对小数据集过拟合。使用一个小数据集，正则化强度设置为0，确保训练后能够到达0的损失值。</li>
</ul>
<h2 id="1-4-检查整个学习过程"><a href="#1-4-检查整个学习过程" class="headerlink" title="1.4 检查整个学习过程"></a>1.4 检查整个学习过程</h2><ul>
<li>跟踪损失值；</li>
<li>跟踪训练集和验证集准确度的变化过程：判断是否过拟合。</li>
<li>第一层可视化：下图是将神经网络第一层的权重可视化的例子。左图中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。右图的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。</li>
</ul>
<p><img src="https://gitee.com/huster_ning/image/raw/master//image/20200708160346.png" srcset="/img/loading.gif" alt=""></p>
<h2 id="1-5-epoch、iteration、batch-size"><a href="#1-5-epoch、iteration、batch-size" class="headerlink" title="1.5 epoch、iteration、batch_size"></a>1.5 epoch、iteration、batch_size</h2><p>神经网络中epoch与iteration是不相等的</p>
<ul>
<li>batch_size：批大小，一次训练选取的样本大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batch_size个样本训练；</li>
<li>iteration：中文翻译为迭代，1个iteration等于使用batch_size个样本训练一次；一个迭代 = 一个正向传播+一个反向传播；</li>
<li>epoch：迭代次数，1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递。</li>
</ul>
<h1 id="2-激活函数（activation-function）"><a href="#2-激活函数（activation-function）" class="headerlink" title="2. 激活函数（activation function）"></a>2. 激活函数（activation function）</h1><p>在神经元节点中线性计算后，然后使用激活函数，激活函数需要是一个非线性函数。使用激活函数是为了将神经元节点输出非线性化，如果只是使用线性函数，那么最后的输出将是几个线性函数的线性组合，那么设计多层的神经网络就变得没有任何意义。</p>
<h2 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h2><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>sigmoid函数将输入压缩到 0 到 1 范围，主要用于二分类，比如logistics回归，如今sigmoid函数很少使用了。</p>
<script type="math/tex; mode=display">
f(x) = \frac {1}{1 + e^{-x}}</script><p>sigmoid函数的导数为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}f^\prime(x) = \frac{e^{-x}}{(1 + e^{-x})^2} \\
= f(x) - (f(x))^2
\end{array}</script><p>sigmoid函数主要有以下两个缺点：</p>
<ul>
<li>输出不是0中心的。这一情况将影响梯度下降的运作。</li>
<li>sigmoid函数饱和使梯度消失。当激活函数的输入非常大或者非常小的时候，梯度几乎为0，那么最后相乘的结果也会接近0.</li>
</ul>
<h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>tanh函数将输入压缩至-1到1的范围，tanh函数是一个简单放大的sigmoid函数：$tanh(x) = 2\sigma(2x) - 1$</p>
<script type="math/tex; mode=display">
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}</script><p>tanh函数的导数：</p>
<script type="math/tex; mode=display">
f(x)^{\prime}=1-(\tanh (x))^{2}</script><p>解决了sigmoid函数的0中心化问题，但梯度消失问题仍然存在。</p>
<h3 id="ReLU函数-Rectified-Linear-Unit"><a href="#ReLU函数-Rectified-Linear-Unit" class="headerlink" title="ReLU函数(Rectified Linear Unit)"></a>ReLU函数(Rectified Linear Unit)</h3><p>ReLU函数（校正线性单元），是目前最常用的激活函数：</p>
<script type="math/tex; mode=display">
f(x) = \max(0, x)</script><p>优点：</p>
<ul>
<li>对于随机梯度下降的收敛有巨大的加速作用。</li>
<li>sigmoid和tanh含有指数运算等耗时操作，ReLU 运算简单，计算速度快。</li>
<li>在正区间解决梯度消失问题。</li>
</ul>
<p>缺点：</p>
<ul>
<li>输出不是0中心化；</li>
<li>某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。例如，learning_rate设置得太高，容易出现这种情况。合理设置learning_rate, 这种情况发生概率会降低。</li>
</ul>
<h3 id="Leaky-ReLU函数"><a href="#Leaky-ReLU函数" class="headerlink" title="Leaky ReLU函数"></a>Leaky ReLU函数</h3><p>为了解决ReLU死亡问题，ReLU函数的改进版本。</p>
<script type="math/tex; mode=display">
f(x) = max(\alpha x, x)</script><p>其中$\alpha$ 是一个非常小的常量。</p>
<h1 id="3-优化"><a href="#3-优化" class="headerlink" title="3. 优化"></a>3. 优化</h1><h2 id="3-1-梯度下降法"><a href="#3-1-梯度下降法" class="headerlink" title="3.1 梯度下降法"></a>3.1 梯度下降法</h2><h3 id="批量梯度下降法（Batch-Gradient-Descent）"><a href="#批量梯度下降法（Batch-Gradient-Descent）" class="headerlink" title="批量梯度下降法（Batch Gradient Descent）"></a>批量梯度下降法（Batch Gradient Descent）</h3><p>更新参数时使用所有的样本进行更新，每迭代一步都需要用到训练集中的所有数据。</p>
<ul>
<li>缺点：样本数量很多时，训练过程很慢。</li>
</ul>
<h3 id="随机梯度下降法（Stochastic-Gradient-Descent）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent）"></a>随机梯度下降法（Stochastic Gradient Descent）</h3><p>随机梯度下降法，每次训练一个样本，计算一个样本的梯度，然后进行参数更新。 </p>
<ul>
<li>优点：训练速度快；</li>
<li>缺点：准确度下降，并不是全局最优。</li>
</ul>
<h3 id="小批量梯度下降法（Mini-batch-Gradient-Descent）"><a href="#小批量梯度下降法（Mini-batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a>小批量梯度下降法（Mini-batch Gradient Descent）</h3><p>每次更新参数时使用小批量的样本。算法训练过程比较快，准确度也比较高。梯度下降会有一些震荡，但整体是下降的，因为mini-batch会出现一些噪音。</p>
<h2 id="3-2-梯度下降优化算法"><a href="#3-2-梯度下降优化算法" class="headerlink" title="3.2 梯度下降优化算法"></a>3.2 梯度下降优化算法</h2><p>梯度下降法的参数更新有多种方法。</p>
<h3 id="1-普通方法"><a href="#1-普通方法" class="headerlink" title="1. 普通方法"></a>1. 普通方法</h3><p>沿着负梯度方向改变参数。</p>
<div class="hljs"><pre><code class="hljs python"><span class="hljs-comment"># 普通更新</span>
W = W - learning_rate * dW</code></pre></div>
<h3 id="2-动量更新（Momentum）"><a href="#2-动量更新（Momentum）" class="headerlink" title="2. 动量更新（Momentum）"></a>2. 动量更新（Momentum）</h3><div class="hljs"><pre><code class="hljs python"><span class="hljs-comment"># 动量更新</span>
v = mu * v - learning_rate * dW
W += v</code></pre></div>
<p>其中v初始化为0，mu被看作动量，一般设为0.9，通产还可以是[0.5, 0.9, 0.95, 0.99]中的一个。</p>
<ul>
<li><p>在梯度方向改变时（比如v为负，dW为负），momentum能够降低参数更新速度，从而减少震荡；</p>
</li>
<li><p>在梯度方向相同时（比如v为负，dW为正），momentum可以加速参数更新， 从而加速收敛。</p>
</li>
</ul>
<p>总而言之，momentum能够加速SGD收敛，抑制震荡。</p>
<h3 id="3-Nesterov动量"><a href="#3-Nesterov动量" class="headerlink" title="3. Nesterov动量"></a>3. Nesterov动量</h3><div class="hljs"><pre><code class="hljs python">v_pre = v <span class="hljs-comment"># 存储备份</span>
v = mu * v - learning_rate * dW
W += -mu * v_pre + (<span class="hljs-number">1</span> + mu) * v</code></pre></div>
<h3 id="4-Adagrad"><a href="#4-Adagrad" class="headerlink" title="4. Adagrad"></a>4. Adagrad</h3><p>Adagrad是一个适应性学习率算法，自适用的为不同参数分配不同的学习率。</p>
<div class="hljs"><pre><code class="hljs python"><span class="hljs-comment"># cache存储梯度的平方和</span>
cache += dW**<span class="hljs-number">2</span>
<span class="hljs-comment"># eps是一个很小的常量，防止出现0, 一般1e-4到1e-8之间</span>
W += -learning_rate * dW / (np.sqrt(cache) + eps)</code></pre></div>
<p>随着迭代的增加，cache增大，学习率变小。因为随着训练的进行，越来越接近最优解，学习率相应变小。</p>
<h3 id="5-RMSprop"><a href="#5-RMSprop" class="headerlink" title="5. RMSprop"></a>5. RMSprop</h3><p>适应性学习率算法，与Adagrad不同的是，不会让学习率单调变小。</p>
<div class="hljs"><pre><code class="hljs python">cache = decay_rate * cache + (<span class="hljs-number">1</span> - decay_rate) * dx ** <span class="hljs-number">2</span>
x += -learning_rate * dx / (np.sqrt(cache) + eps)</code></pre></div>
<p>decay_rate常用的值[0.9, 0.99, 0.999]</p>
<h3 id="6-Adam"><a href="#6-Adam" class="headerlink" title="6. Adam"></a>6. Adam</h3><p>看起来像RMSprop的动量版。</p>
<div class="hljs"><pre><code class="hljs python">m = beta1*m + (<span class="hljs-number">1</span>‐beta1)*dx
v = beta2*v + (<span class="hljs-number">1</span>‐beta2)*(dx**<span class="hljs-number">2</span>)
x += ‐ learning_rate * m / (np.sqrt(v) + eps)</code></pre></div>
<p>推荐参数值：eps=1e-8, beta1=0.9, beta2=0.999。</p>
<h2 id="3-3-学习率退火"><a href="#3-3-学习率退火" class="headerlink" title="3.3 学习率退火"></a>3.3 学习率退火</h2><p>学习率可能过大，逐步衰减学习率。</p>
<ul>
<li>随步数衰减：每进行几个周期就降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1；</li>
<li>指数衰减：$\alpha = \alpha_0 e^{-kt}$, 其中t 为迭代次数，k是超参数；</li>
<li>1/t 衰减: $ \alpha = \frac{\alpha_0}{1+ kt}$.</li>
</ul>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/深度学习/">深度学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/深度学习/">深度学习</a>
                    
                      <a class="hover-with-bg" href="/tags/神经网络/">神经网络</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2020/07/09/卷积神经网络/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">卷积神经网络</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/07/05/聚类与降维/">
                        <span class="hidden-mobile">聚类与降维</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
              <!-- Comments -->
              <div class="comments" id="comments">
                
                
  <div id="vcomments"></div>
  <script type="text/javascript">
    function loadValine() {
      addScript('https://cdn.staticfile.org/valine/1.4.14/Valine.min.js', function () {
        new Valine({
          el: "#vcomments",
          app_id: "RIPrHIBMNeTH5hqWK3v3S3tY-gzGzoHsz",
          app_key: "v3JUvWKIM4IppVhBGgMhs66O",
          placeholder: "说点什么",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: false,
          recordIP: false,
          serverURLs: "https://riprhibm.lc-cn-n1-shared.com",
        });
      });
    }
    createObserver(loadValine, 'vcomments');
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://valine.js.org" rel="nofollow noopener">comments
      powered by Valine.</a></noscript>


              </div>
            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


    
  <!-- 备案信息 -->
  <div class="beian">
    <a href="http://beian.miit.gov.cn/" target="_blank"
       rel="nofollow noopener">湘ICP证20011128号</a>
    
      <a
        href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=20011128"
        rel="nofollow noopener"
        class="beian-police"
        target="_blank"
      >
        <span class="beian-police-sep">&nbsp;|&nbsp;</span>
        
          <img src="/img/police_beian.png" srcset="/img/loading.gif" alt="police-icon" />
        
        <span>湘ICP备20011128号</span>
      </a>
     
  </div>


    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "神经网络&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-svg.js" ></script>

  
















</body>
</html>
